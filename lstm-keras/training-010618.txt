dmin-u4610248@cecs-039843:~/Documents/learning-sequences/lstm-keras$ python3 create-weights.py 
/home/admin-u4610248/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Total Characters:  144431
Total Vocab:  46
Total Patterns:  144331
Small(S) OR Large(L): l
(144331, 100, 1) (144331, 45)
Epoch 1/20
2018-06-01 08:45:52.469709: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-01 08:45:52.469734: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-01 08:45:52.469740: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-01 08:45:52.469746: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-01 08:45:52.469750: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
144331/144331 [==============================] - 3303s 23ms/step - loss: 2.8122

Epoch 00001: loss improved from inf to 2.81222, saving model to weights-improvement-01-2.8122-bigger.hdf5
Epoch 2/20
144331/144331 [==============================] - 3365s 23ms/step - loss: 2.4013

Epoch 00002: loss improved from 2.81222 to 2.40135, saving model to weights-improvement-02-2.4013-bigger.hdf5
Epoch 3/20
144331/144331 [==============================] - 3317s 23ms/step - loss: 2.1343

Epoch 00003: loss improved from 2.40135 to 2.13427, saving model to weights-improvement-03-2.1343-bigger.hdf5
Epoch 4/20
144331/144331 [==============================] - 3305s 23ms/step - loss: 1.9589

Epoch 00004: loss improved from 2.13427 to 1.95891, saving model to weights-improvement-04-1.9589-bigger.hdf5
Epoch 5/20
144331/144331 [==============================] - 3306s 23ms/step - loss: 1.8245

Epoch 00005: loss improved from 1.95891 to 1.82448, saving model to weights-improvement-05-1.8245-bigger.hdf5
Epoch 6/20
144331/144331 [==============================] - 3305s 23ms/step - loss: 1.7166

Epoch 00006: loss improved from 1.82448 to 1.71661, saving model to weights-improvement-06-1.7166-bigger.hdf5
Epoch 7/20
144331/144331 [==============================] - 3314s 23ms/step - loss: 1.6199

Epoch 00007: loss improved from 1.71661 to 1.61990, saving model to weights-improvement-07-1.6199-bigger.hdf5
Epoch 8/20
144331/144331 [==============================] - 3306s 23ms/step - loss: 1.5392

Epoch 00008: loss improved from 1.61990 to 1.53917, saving model to weights-improvement-08-1.5392-bigger.hdf5
Epoch 9/20
144331/144331 [==============================] - 3305s 23ms/step - loss: 1.4654

Epoch 00009: loss improved from 1.53917 to 1.46537, saving model to weights-improvement-09-1.4654-bigger.hdf5
Epoch 10/20
144331/144331 [==============================] - 3305s 23ms/step - loss: 1.3948

Epoch 00010: loss improved from 1.46537 to 1.39483, saving model to weights-improvement-10-1.3948-bigger.hdf5
Epoch 11/20
144331/144331 [==============================] - 3305s 23ms/step - loss: 1.3278

Epoch 00011: loss improved from 1.39483 to 1.32777, saving model to weights-improvement-11-1.3278-bigger.hdf5
Epoch 12/20
144331/144331 [==============================] - 3304s 23ms/step - loss: 1.2654

Epoch 00012: loss improved from 1.32777 to 1.26544, saving model to weights-improvement-12-1.2654-bigger.hdf5
Epoch 13/20
144331/144331 [==============================] - 3303s 23ms/step - loss: 1.2092

Epoch 00013: loss improved from 1.26544 to 1.20920, saving model to weights-improvement-13-1.2092-bigger.hdf5
Epoch 14/20
144331/144331 [==============================] - 3303s 23ms/step - loss: 1.1581

Epoch 00014: loss improved from 1.20920 to 1.15811, saving model to weights-improvement-14-1.1581-bigger.hdf5
Epoch 15/20
144331/144331 [==============================] - 3302s 23ms/step - loss: 1.1099

Epoch 00015: loss improved from 1.15811 to 1.10986, saving model to weights-improvement-15-1.1099-bigger.hdf5
Epoch 16/20
144331/144331 [==============================] - 3302s 23ms/step - loss: 1.0685

Epoch 00016: loss improved from 1.10986 to 1.06854, saving model to weights-improvement-16-1.0685-bigger.hdf5
Epoch 17/20
144331/144331 [==============================] - 3301s 23ms/step - loss: 1.0271

Epoch 00017: loss improved from 1.06854 to 1.02707, saving model to weights-improvement-17-1.0271-bigger.hdf5
Epoch 18/20
144331/144331 [==============================] - 3301s 23ms/step - loss: 0.9938

Epoch 00018: loss improved from 1.02707 to 0.99383, saving model to weights-improvement-18-0.9938-bigger.hdf5
Epoch 19/20
144331/144331 [==============================] - 3300s 23ms/step - loss: 0.9581

Epoch 00019: loss improved from 0.99383 to 0.95811, saving model to weights-improvement-19-0.9581-bigger.hdf5
Epoch 20/20
144331/144331 [==============================] - 3303s 23ms/step - loss: 0.9309

Epoch 00020: loss improved from 0.95811 to 0.93093, saving model to weights-improvement-20-0.9309-bigger.hdf5

